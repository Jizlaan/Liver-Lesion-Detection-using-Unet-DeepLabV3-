# Liver-Lesion-Detection-using-Unet-DeepLabV3+
import os
import cv2
import numpy as np
from sklearn.model_selection import KFold
import tensorflow as tf
from tensorflow.keras.utils import Sequence
from tensorflow.keras.layers import Conv2D, Input, BatchNormalization, Activation
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import Precision, Recall
import matplotlib.pyplot as plt

# Constants
IMAGE_DIR = '/raid/lits_dataset/train_images/train_images'
MASK_DIR = '/raid/lits_dataset/train_masks/train_masks'
IMAGE_SIZE = (256, 256)
BATCH_SIZE = 16
EPOCHS = 10
NUM_FOLDS = 10

# Dice loss function
def dice_loss(y_true, y_pred, smooth=1e-6):
    y_true_f = tf.reshape(y_true, [-1])
    y_pred_f = tf.reshape(y_pred, [-1])
    intersection = tf.reduce_sum(y_true_f * y_pred_f)
    return 1 - (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)

# Data Generator
class SegmentationDataGenerator(Sequence):
    def _init_(self, image_dir, mask_dir, file_list, batch_size=16,
                 image_size=(256, 256), shuffle=True, augment_fn=None):
        self.image_dir = image_dir
        self.mask_dir = mask_dir
        self.file_list = list(file_list)
        self.batch_size = batch_size
        self.image_size = image_size
        self.shuffle = shuffle
        self.augment_fn = augment_fn
        self.on_epoch_end()

    def _len_(self):
        return int(np.ceil(len(self.file_list) / self.batch_size))

    def _getitem_(self, idx):
        batch_files = self.file_list[idx * self.batch_size:(idx + 1) * self.batch_size]
        imgs, masks = [], []

        for fname in batch_files:
            img_path = os.path.join(self.image_dir, fname)
            mask_path = os.path.join(self.mask_dir, fname)

            # Load image and mask
            img = cv2.imread(img_path, cv2.IMREAD_COLOR)
            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

            # Resize
            img = cv2.resize(img, self.image_size)
            mask = cv2.resize(mask, self.image_size)

            # Normalize image
            img = img.astype(np.float32) / 255.0

            # Binarize mask and expand channel dimension
            mask = (mask > 0).astype(np.float32)
            mask = np.expand_dims(mask, axis=-1)  # (H, W, 1)

            # Apply augmentation if provided
            if self.augment_fn:
                img, mask = self.augment_fn(img, mask)

            imgs.append(img)
            masks.append(mask)

        X = np.stack(imgs, axis=0)
        y = np.stack(masks, axis=0)

        return X, y

    def on_epoch_end(self):
        if self.shuffle:
            np.random.shuffle(self.file_list)

# Optional simple augmentation function
def simple_augment(img, mask):
    if np.random.rand() < 0.5:
        img = np.fliplr(img)
        mask = np.fliplr(mask)
    return img, mask

# Model definition (modified architecture)
def create_deeplabv3plus(input_shape=(256, 256, 3), num_classes=1):
    inputs = Input(shape=input_shape)

    x = Conv2D(64, 3, padding='same', activation='relu')(inputs)
    x = BatchNormalization()(x)

    x = Conv2D(128, 3, padding='same', activation='relu')(x)
    x = BatchNormalization()(x)

    x = Conv2D(256, 3, padding='same', activation='relu')(x)
    x = BatchNormalization()(x)

    outputs = Conv2D(num_classes, 1, activation='sigmoid')(x)

    model = Model(inputs, outputs)
    return model

# Get all image filenames
all_files = os.listdir(IMAGE_DIR)
all_files = [f for f in all_files if f.endswith(('.png', '.jpg', '.jpeg'))]

# Setup KFold
kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)

fold_no = 1
for train_index, val_index in kf.split(all_files):
    print(f'Fold {fold_no} - Training on {len(train_index)} samples, validating on {len(val_index)} samples')

    train_files = [all_files[i] for i in train_index]
    val_files = [all_files[i] for i in val_index]

    train_gen = SegmentationDataGenerator(IMAGE_DIR, MASK_DIR, train_files,
                                          batch_size=BATCH_SIZE,
                                          image_size=IMAGE_SIZE,
                                          shuffle=True,
                                          augment_fn=simple_augment)

    val_gen = SegmentationDataGenerator(IMAGE_DIR, MASK_DIR, val_files,
                                        batch_size=BATCH_SIZE,
                                        image_size=IMAGE_SIZE,
                                        shuffle=False)

    model = create_deeplabv3plus(input_shape=(*IMAGE_SIZE, 3), num_classes=1)
    model.compile(optimizer=Adam(),
                  loss=dice_loss,
                  metrics=['accuracy', Precision(), Recall()])

    steps_per_epoch = len(train_gen)
    validation_steps = len(val_gen)

    model.fit(train_gen,
              validation_data=val_gen,
              epochs=EPOCHS,
              steps_per_epoch=steps_per_epoch,
              validation_steps=validation_steps)

    fold_no += 1
